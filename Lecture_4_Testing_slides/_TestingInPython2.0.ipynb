{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9f58cc6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Testing in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93806753",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda\n",
    "\n",
    "* Testing pyramid\n",
    "* Test runner characteristics\n",
    "* Test structure\n",
    "* Tests best practices\n",
    "* Unittest\n",
    "* Pytest\n",
    "* RobotFramework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca41b836",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Testing pyramid\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9f0098",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![test_pyramid.png](images/test_pyramid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3625d69",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Testing pyramid vs Runners\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bed546",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![test_pyramid.png](images/test_pyramid2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aeb17f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Test runner characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4c44fa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. **Structure**: test case structure, test naming, config files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a26f08",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. **Launcher**: run parameters, command line or CI/CD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8c5f15",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. **Setup/teardown and fixtures**: scope, parameters, fixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b2e8f3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "4. **Parametrization**: direct, indirect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af045a4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "5. **Asserts**: flexible, informative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8c74d5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "6. **Reports**: single file, available online"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d60a92d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "7. **Userful features**: marks/tags/keywords, mocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c0836e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "8. **Popular methodolodies**: data driven testing, business driven testing, keyword driven testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f782b8c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "9. **Plugins**: additional capabilities (e.g. reruns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcec32d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "10. **Concurrent execution**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b23742",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Test structure\n",
    "\n",
    "### Arrange -> Act -> Assert = Given -> When -> Then\n",
    "\n",
    "* **Arrange (Given)**\n",
    "    * setup method, fixture, factory pattern\n",
    "* **Act (When)**\n",
    "    * one simple action\n",
    "* **Assert (Then)**\n",
    "    * one or several (not many) asserts that check the action above\n",
    "* **Cleanup (optional)**\n",
    "    * for convenience, not precondition!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7b3387",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tests best practices\n",
    "\n",
    "* ***Tests MUST be independent***\n",
    "* Tests must be informative\n",
    "    * name\n",
    "    * assert\n",
    "* Tests must be simple\n",
    "    * check one thing at a time\n",
    "* Tests must not have:\n",
    "    * inner logic (ifs, loops...)\n",
    "    * access by indexes\n",
    "* Tests must be easily readable\n",
    "    * follow Arrange/Act/Assert (Given/When/Then) approach\n",
    "    * group steps in needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd572a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unittest\n",
    "\n",
    "* a spiritual successor of JUnit\n",
    "* focuses around **unittest.TestCase** class\n",
    "* can be run from the module, but preferably to be run from a separate file\n",
    "* tests are methods in a class and must start with test_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a75e9b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f445473",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class SampleTestCase(unittest.TestCase):                      # Must inherit from unittest.TestCase, any class name\n",
    "     \n",
    "    def test_first_test(self):                                # Must start with test_\n",
    "        number = 42\n",
    "        self.assertEqual(number, 42)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)  # Just for Jupyter Notebook\n",
    "    # unittest.main()                                         # Please, use this instead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51825b40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# test_filename vs filename_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "973a89da",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows\n",
      " Volume Serial Number is B8A6-CAAE\n",
      "\n",
      " Directory of C:\\_PythonProjects\\STA-with-Python-1\\Lecture_4_Testing_slides\n",
      "\n",
      "12.02.2022  07:35    <DIR>          .\n",
      "12.02.2022  07:35    <DIR>          ..\n",
      "12.02.2022  07:20    <DIR>          .ipynb_checkpoints\n",
      "11.02.2022  15:15    <DIR>          badger\n",
      "11.02.2022  17:15    <DIR>          images\n",
      "12.02.2022  06:56                71 more_code.py\n",
      "12.02.2022  07:11               253 more_code_test.py\n",
      "12.02.2022  06:56                71 some_code.py\n",
      "12.02.2022  07:11               253 some_code_test.py\n",
      "12.02.2022  07:11               253 test_more_code.py\n",
      "12.02.2022  07:11               253 test_some_code.py\n",
      "12.02.2022  07:25           617ÿ150 _TestingInPython.ipynb\n",
      "12.02.2022  07:35            13ÿ660 _TestingInPython2.0.ipynb\n",
      "12.02.2022  07:21    <DIR>          __pycache__\n",
      "               8 File(s)        631ÿ964 bytes\n",
      "               6 Dir(s)  362ÿ787ÿ581ÿ952 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c815ec0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Writing unit tests for a module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9901fe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# some_code.py\n",
    "\n",
    "def get_square(number):\n",
    "    square = number ** 2\n",
    "    \n",
    "    return square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cf9a37",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# test_some_code.py\n",
    "\n",
    "import unittest\n",
    "import some_code\n",
    "\n",
    "class TestSomeCode(unittest.TestCase):\n",
    "    \n",
    "    def test_square_positive_number(self):\n",
    "        square = some_code.get_square(2)\n",
    "        \n",
    "        self.assertEqual(square, 4)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a171e2d7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!py test_some_code.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa1144e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# unittest is a runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d21b9f85",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: python.exe -m unittest [-h] [-v] [-q] [--locals] [-f] [-c] [-b]\n",
      "                              [-k TESTNAMEPATTERNS]\n",
      "                              [tests ...]\n",
      "\n",
      "positional arguments:\n",
      "  tests                a list of any number of test modules, classes and test\n",
      "                       methods.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help           show this help message and exit\n",
      "  -v, --verbose        Verbose output\n",
      "  -q, --quiet          Quiet output\n",
      "  --locals             Show local variables in tracebacks\n",
      "  -f, --failfast       Stop on first fail or error\n",
      "  -c, --catch          Catch Ctrl-C and display results so far\n",
      "  -b, --buffer         Buffer stdout and stderr during tests\n",
      "  -k TESTNAMEPATTERNS  Only run tests which match the given substring\n",
      "\n",
      "Examples:\n",
      "  python.exe -m unittest test_module               - run tests from test_module\n",
      "  python.exe -m unittest module.TestClass          - run tests from module.TestClass\n",
      "  python.exe -m unittest module.Class.test_method  - run specified test method\n",
      "  python.exe -m unittest path/to/test_file.py      - run tests from test_file.py\n",
      "\n",
      "usage: python.exe -m unittest discover [-h] [-v] [-q] [--locals] [-f] [-c]\n",
      "                                       [-b] [-k TESTNAMEPATTERNS] [-s START]\n",
      "                                       [-p PATTERN] [-t TOP]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -v, --verbose         Verbose output\n",
      "  -q, --quiet           Quiet output\n",
      "  --locals              Show local variables in tracebacks\n",
      "  -f, --failfast        Stop on first fail or error\n",
      "  -c, --catch           Catch Ctrl-C and display results so far\n",
      "  -b, --buffer          Buffer stdout and stderr during tests\n",
      "  -k TESTNAMEPATTERNS   Only run tests which match the given substring\n",
      "  -s START, --start-directory START\n",
      "                        Directory to start discovery ('.' default)\n",
      "  -p PATTERN, --pattern PATTERN\n",
      "                        Pattern to match tests ('test*.py' default)\n",
      "  -t TOP, --top-level-directory TOP\n",
      "                        Top level directory of project (defaults to start\n",
      "                        directory)\n",
      "\n",
      "For test discovery all test modules must be importable from the top level\n",
      "directory of the project.\n"
     ]
    }
   ],
   "source": [
    "!py -m unittest --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c416310c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Verbose mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b474edbe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_square_positive_number (test_more_code.SomeCodeTestCase) ... ok\n",
      "test_square_positive_number (test_some_code.TestSomeCode) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.000s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!py -m unittest -v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb105b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Setup and teardown for a test/a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5fbc042",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a class setup.\n",
      "This is a test setup.\n",
      "This is Test #1\n",
      "This is a test teardown.\n",
      "This is a test setup.\n",
      "This is Test #2\n",
      "This is a test teardown.\n",
      "This is a class teardown.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.002s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class SampleTestCase(unittest.TestCase):\n",
    "     \n",
    "    def setUp(self):                                          # Must be cameCase\n",
    "        print(\"This is a test setup.\")\n",
    "        self.number1 = 42\n",
    "    \n",
    "    def tearDown(self):                                       # Must be cameCase\n",
    "        print(\"This is a test teardown.\")\n",
    "        self.number1 = 0\n",
    "        \n",
    "    @classmethod                                              # Must have this decorator\n",
    "    def setUpClass(cls):                                      # Must be cameCase\n",
    "        print(\"This is a class setup.\")\n",
    "        \n",
    "    @classmethod                                              # Must have this decorator\n",
    "    def tearDownClass(cls):                                   # Must be cameCase\n",
    "        print(\"This is a class teardown.\")\n",
    "\n",
    "    def test_answer_is_42(self):\n",
    "        print(\"This is Test #1\")\n",
    "        self.assertEqual(self.number1, 42)\n",
    "        \n",
    "    def test_answer_is_not_666(self):\n",
    "        print(\"This is Test #2\")\n",
    "        self.assertNotEqual(self.number1, 666)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)  # Just for Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f23d5fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Failed tests and skipped tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74297693",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fs\n",
      "======================================================================\n",
      "FAIL: test_answer_is_666 (__main__.SampleTestCase)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ilia_Serdiuk\\AppData\\Local\\Temp\\ipykernel_13844\\3940538478.py\", line 9, in test_answer_is_666\n",
      "    self.assertEqual(self.number1, 666)\n",
      "AssertionError: 42 != 666\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.001s\n",
      "\n",
      "FAILED (failures=1, skipped=1)\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class SampleTestCase(unittest.TestCase):\n",
    "     \n",
    "    def setUp(self):\n",
    "        self.number1 = 42\n",
    "\n",
    "    def test_answer_is_666(self):\n",
    "        self.assertEqual(self.number1, 666)\n",
    "        \n",
    "    @unittest.skip(\"I don't like this test\")\n",
    "    def test_answer_is_not_666(self):\n",
    "        number2 = 666\n",
    "        self.assertNotEqual(self.number1, number2)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)  # Just for Jupyter Notebook\n",
    "    # unittest.main()                                         # Please, use this instead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6e4862",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unittest summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b07b1be",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Advantages**:\n",
    "    * in-build Python module, i.e. no installation needed\n",
    "    * setup/teardown fixtures for tests/classes are supported\n",
    "    * easy to write and to start\n",
    "    * mocks are available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979cc201",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Disadvantages**:\n",
    "    * no test/fixture parametrization\n",
    "    * no data driven testing\n",
    "    * no reports out-of-the-box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5713b81",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pytest\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc2fc62",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A third-party solution, developed since 2007\n",
    "* Can be run via commandline or from IDE\n",
    "* A test is any function that starts with test_ in a file that starts with test_\n",
    "* Flexible, customizable, well documented and supported"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cef8b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pytest is a runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bae2e9b9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: pytest [options] [file_or_dir] [file_or_dir] [...]\n",
      "\n",
      "positional arguments:\n",
      "  file_or_dir\n",
      "\n",
      "general:\n",
      "  -k EXPRESSION         only run tests which match the given substring\n",
      "                        expression. An expression is a python evaluatable\n",
      "                        expression where all names are substring-matched against\n",
      "                        test names and their parent classes. Example: -k\n",
      "                        'test_method or test_other' matches all test functions\n",
      "                        and classes whose name contains 'test_method' or\n",
      "                        'test_other', while -k 'not test_method' matches those\n",
      "                        that don't contain 'test_method' in their names. -k 'not\n",
      "                        test_method and not test_other' will eliminate the\n",
      "                        matches. Additionally keywords are matched to classes\n",
      "                        and functions containing extra names in their\n",
      "                        'extra_keyword_matches' set, as well as functions which\n",
      "                        have names assigned directly to them. The matching is\n",
      "                        case-insensitive.\n",
      "  -m MARKEXPR           only run tests matching given mark expression.\n",
      "                        For example: -m 'mark1 and not mark2'.\n",
      "  --markers             show markers (builtin, plugin and per-project ones).\n",
      "  -x, --exitfirst       exit instantly on first error or failed test.\n",
      "  --fixtures, --funcargs\n",
      "                        show available fixtures, sorted by plugin appearance\n",
      "                        (fixtures with leading '_' are only shown with '-v')\n",
      "  --fixtures-per-test   show fixtures per test\n",
      "  --pdb                 start the interactive Python debugger on errors or\n",
      "                        KeyboardInterrupt.\n",
      "  --pdbcls=modulename:classname\n",
      "                        start a custom interactive Python debugger on errors.\n",
      "                        For example:\n",
      "                        --pdbcls=IPython.terminal.debugger:TerminalPdb\n",
      "  --trace               Immediately break when running each test.\n",
      "  --capture=method      per-test capturing method: one of fd|sys|no|tee-sys.\n",
      "  -s                    shortcut for --capture=no.\n",
      "  --runxfail            report the results of xfail tests as if they were not\n",
      "                        marked\n",
      "  --lf, --last-failed   rerun only the tests that failed at the last run (or all\n",
      "                        if none failed)\n",
      "  --ff, --failed-first  run all tests, but run the last failures first.\n",
      "                        This may re-order tests and thus lead to repeated\n",
      "                        fixture setup/teardown.\n",
      "  --nf, --new-first     run tests from new files first, then the rest of the\n",
      "                        tests sorted by file mtime\n",
      "  --cache-show=[CACHESHOW]\n",
      "                        show cache contents, don't perform collection or tests.\n",
      "                        Optional argument: glob (default: '*').\n",
      "  --cache-clear         remove all cache contents at start of test run.\n",
      "  --lfnf={all,none}, --last-failed-no-failures={all,none}\n",
      "                        which tests to run with no previously (known) failures.\n",
      "  --sw, --stepwise      exit on test failure and continue from last failing test\n",
      "                        next time\n",
      "  --sw-skip, --stepwise-skip\n",
      "                        ignore the first failing test but stop on the next\n",
      "                        failing test\n",
      "  --allure-severities=SEVERITIES_SET\n",
      "                        Comma-separated list of severity names.\n",
      "                        Tests only with these severities will be run.\n",
      "                        Possible values are: blocker, critical, normal, minor,\n",
      "                        trivial.\n",
      "  --allure-epics=EPICS_SET\n",
      "                        Comma-separated list of epic names.\n",
      "                        Run tests that have at least one of the specified\n",
      "                        feature labels.\n",
      "  --allure-features=FEATURES_SET\n",
      "                        Comma-separated list of feature names.\n",
      "                        Run tests that have at least one of the specified\n",
      "                        feature labels.\n",
      "  --allure-stories=STORIES_SET\n",
      "                        Comma-separated list of story names.\n",
      "                        Run tests that have at least one of the specified story\n",
      "                        labels.\n",
      "  --allure-ids=IDS_SET  Comma-separated list of IDs.\n",
      "                        Run tests that have at least one of the specified id\n",
      "                        labels.\n",
      "  --allure-link-pattern=LINK_TYPE:LINK_PATTERN\n",
      "                        Url pattern for link type. Allows short links in test,\n",
      "                        like 'issue-1'. Text will be formatted to full url with\n",
      "                        python\n",
      "                        str.format().\n",
      "\n",
      "reporting:\n",
      "  --durations=N         show N slowest setup/test durations (N=0 for all).\n",
      "  --durations-min=N     Minimal duration in seconds for inclusion in slowest\n",
      "                        list. Default 0.005\n",
      "  -v, --verbose         increase verbosity.\n",
      "  --no-header           disable header\n",
      "  --no-summary          disable summary\n",
      "  -q, --quiet           decrease verbosity.\n",
      "  --verbosity=VERBOSE   set verbosity. Default is 0.\n",
      "  -r chars              show extra test summary info as specified by chars:\n",
      "                        (f)ailed, (E)rror, (s)kipped, (x)failed, (X)passed,\n",
      "                        (p)assed, (P)assed with output, (a)ll except passed\n",
      "                        (p/P), or (A)ll. (w)arnings are enabled by default (see\n",
      "                        --disable-warnings), 'N' can be used to reset the list.\n",
      "                        (default: 'fE').\n",
      "  --disable-warnings, --disable-pytest-warnings\n",
      "                        disable warnings summary\n",
      "  -l, --showlocals      show locals in tracebacks (disabled by default).\n",
      "  --tb=style            traceback print mode (auto/long/short/line/native/no).\n",
      "  --show-capture={no,stdout,stderr,log,all}\n",
      "                        Controls how captured stdout/stderr/log is shown on\n",
      "                        failed tests. Default is 'all'.\n",
      "  --full-trace          don't cut any tracebacks (default is to cut).\n",
      "  --color=color         color terminal output (yes/no/auto).\n",
      "  --code-highlight={yes,no}\n",
      "                        Whether code should be highlighted (only if --color is\n",
      "                        also enabled)\n",
      "  --pastebin=mode       send failed|all info to bpaste.net pastebin service.\n",
      "  --junit-xml=path      create junit-xml style report file at given path.\n",
      "  --junit-prefix=str    prepend prefix to classnames in junit-xml output\n",
      "  --html=path           create html report file at given path.\n",
      "  --self-contained-html\n",
      "                        create a self-contained html file containing all\n",
      "                        necessary styles, scripts, and images - this means that\n",
      "                        the report may not render or function where CSP\n",
      "                        restrictions are in place (see\n",
      "                        https://developer.mozilla.org/docs/Web/Security/CSP)\n",
      "  --css=path            append given css file content to report style file.\n",
      "  --instafail           show failures and errors instantly as they occur\n",
      "                        (disabled by default).\n",
      "\n",
      "pytest-warnings:\n",
      "  -W PYTHONWARNINGS, --pythonwarnings=PYTHONWARNINGS\n",
      "                        set which warnings to report, see -W option of python\n",
      "                        itself.\n",
      "  --maxfail=num         exit after first num failures or errors.\n",
      "  --strict-config       any warnings encountered while parsing the `pytest`\n",
      "                        section of the configuration file raise errors.\n",
      "  --strict-markers      markers not registered in the `markers` section of the\n",
      "                        configuration file raise errors.\n",
      "  --strict              (deprecated) alias to --strict-markers.\n",
      "  -c file               load configuration from `file` instead of trying to\n",
      "                        locate one of the implicit configuration files.\n",
      "  --continue-on-collection-errors\n",
      "                        Force test execution even if collection errors occur.\n",
      "  --rootdir=ROOTDIR     Define root directory for tests. Can be relative path:\n",
      "                        'root_dir', './root_dir', 'root_dir/another_dir/';\n",
      "                        absolute path: '/home/user/root_dir'; path with\n",
      "                        variables: '$HOME/root_dir'.\n",
      "\n",
      "collection:\n",
      "  --collect-only, --co  only collect tests, don't execute them.\n",
      "  --pyargs              try to interpret all arguments as python packages.\n",
      "  --ignore=path         ignore path during collection (multi-allowed).\n",
      "  --ignore-glob=path    ignore path pattern during collection (multi-allowed).\n",
      "  --deselect=nodeid_prefix\n",
      "                        deselect item (via node id prefix) during collection\n",
      "                        (multi-allowed).\n",
      "  --confcutdir=dir      only load conftest.py's relative to specified dir.\n",
      "  --noconftest          Don't load any conftest.py files.\n",
      "  --keep-duplicates     Keep duplicate tests.\n",
      "  --collect-in-virtualenv\n",
      "                        Don't ignore tests in a local virtualenv directory\n",
      "  --import-mode={prepend,append,importlib}\n",
      "                        prepend/append to sys.path when importing test modules\n",
      "                        and conftest files, default is to prepend.\n",
      "  --doctest-modules     run doctests in all .py modules\n",
      "  --doctest-report={none,cdiff,ndiff,udiff,only_first_failure}\n",
      "                        choose another output format for diffs on doctest\n",
      "                        failure\n",
      "  --doctest-glob=pat    doctests file matching pattern, default: test*.txt\n",
      "  --doctest-ignore-import-errors\n",
      "                        ignore doctest ImportErrors\n",
      "  --doctest-continue-on-failure\n",
      "                        for a given doctest, continue to run after the first\n",
      "                        failure\n",
      "\n",
      "test session debugging and configuration:\n",
      "  --basetemp=dir        base temporary directory for this test run.(warning:\n",
      "                        this directory is removed if it exists)\n",
      "  -V, --version         display pytest version and information about\n",
      "                        plugins.When given twice, also display information about\n",
      "                        plugins.\n",
      "  -h, --help            show help message and configuration info\n",
      "  -p name               early-load given plugin module name or entry point\n",
      "                        (multi-allowed).\n",
      "                        To avoid loading of plugins, use the `no:` prefix, e.g.\n",
      "                        `no:doctest`.\n",
      "  --trace-config        trace considerations of conftest.py files.\n",
      "  --debug               store internal tracing debug information in\n",
      "                        'pytestdebug.log'.\n",
      "  -o OVERRIDE_INI, --override-ini=OVERRIDE_INI\n",
      "                        override ini option with \"option=value\" style, e.g. `-o\n",
      "                        xfail_strict=True -o cache_dir=cache`.\n",
      "  --assert=MODE         Control assertion debugging tools.\n",
      "                        'plain' performs no assertion debugging.\n",
      "                        'rewrite' (the default) rewrites assert statements in\n",
      "                        test modules on import to provide assert expression\n",
      "                        information.\n",
      "  --setup-only          only setup fixtures, do not execute tests.\n",
      "  --setup-show          show setup of fixtures while executing tests.\n",
      "  --setup-plan          show what fixtures and tests would be executed but don't\n",
      "                        execute anything.\n",
      "\n",
      "logging:\n",
      "  --log-level=LEVEL     level of messages to catch/display.\n",
      "                        Not set by default, so it depends on the root/parent log\n",
      "                        handler's effective level, where it is \"WARNING\" by\n",
      "                        default.\n",
      "  --log-format=LOG_FORMAT\n",
      "                        log format as used by the logging module.\n",
      "  --log-date-format=LOG_DATE_FORMAT\n",
      "                        log date format as used by the logging module.\n",
      "  --log-cli-level=LOG_CLI_LEVEL\n",
      "                        cli logging level.\n",
      "  --log-cli-format=LOG_CLI_FORMAT\n",
      "                        log format as used by the logging module.\n",
      "  --log-cli-date-format=LOG_CLI_DATE_FORMAT\n",
      "                        log date format as used by the logging module.\n",
      "  --log-file=LOG_FILE   path to a file when logging will be written to.\n",
      "  --log-file-level=LOG_FILE_LEVEL\n",
      "                        log file logging level.\n",
      "  --log-file-format=LOG_FILE_FORMAT\n",
      "                        log format as used by the logging module.\n",
      "  --log-file-date-format=LOG_FILE_DATE_FORMAT\n",
      "                        log date format as used by the logging module.\n",
      "  --log-auto-indent=LOG_AUTO_INDENT\n",
      "                        Auto-indent multiline messages passed to the logging\n",
      "                        module. Accepts true|on, false|off or an integer.\n",
      "\n",
      "reporting:\n",
      "  --alluredir=DIR       Generate Allure report in the specified directory (may\n",
      "                        not exist)\n",
      "  --clean-alluredir     Clean alluredir folder if it exists\n",
      "  --allure-no-capture   Do not attach pytest captured logging/stdout/stderr to\n",
      "                        report\n",
      "  --inversion=INVERSION\n",
      "                        Run tests not in testplan\n",
      "\n",
      "re-run failing tests to eliminate flaky failures:\n",
      "  --only-rerun=ONLY_RERUN\n",
      "                        If passed, only rerun errors matching the regex\n",
      "                        provided. Pass this flag multiple times to accumulate a\n",
      "                        list of regexes to match\n",
      "  --reruns=RERUNS       number of times to re-run failed tests. defaults to 0.\n",
      "  --reruns-delay=RERUNS_DELAY\n",
      "                        add time (seconds) delay between reruns.\n",
      "\n",
      "qase:\n",
      "  --qase                Use Qase TMS\n",
      "  --qase-api-token=QS_API_TOKEN\n",
      "                        Api token for Qase TMS\n",
      "  --qase-project=QS_PROJECT_CODE\n",
      "                        Project code in Qase TMS\n",
      "  --qase-testrun=QS_TESTRUN_ID\n",
      "                        Testrun ID in Qase TMS\n",
      "  --qase-testplan=QS_TESTPLAN_ID\n",
      "                        Testplan ID in Qase TMS\n",
      "  --qase-new-run        Create new testrun, if no testrun id provided\n",
      "  --qase-complete-run   Complete run after all tests are finished\n",
      "  --qase-debug          Prints additional output of plugin\n",
      "\n",
      "custom options:\n",
      "  --metadata=key value  additional metadata.\n",
      "  --metadata-from-json=METADATA_FROM_JSON\n",
      "                        additional metadata from a json string.\n",
      "\n",
      "[pytest] ini-options in the first pytest.ini|tox.ini|setup.cfg file found:\n",
      "\n",
      "  markers (linelist):   markers for test functions\n",
      "  empty_parameter_set_mark (string):\n",
      "                        default marker for empty parametersets\n",
      "  norecursedirs (args): directory patterns to avoid for recursion\n",
      "  testpaths (args):     directories to search for tests when no files or\n",
      "                        directories are given in the command line.\n",
      "  filterwarnings (linelist):\n",
      "                        Each line specifies a pattern for\n",
      "                        warnings.filterwarnings. Processed after\n",
      "                        -W/--pythonwarnings.\n",
      "  usefixtures (args):   list of default fixtures to be used with this project\n",
      "  python_files (args):  glob-style file patterns for Python test module\n",
      "                        discovery\n",
      "  python_classes (args):\n",
      "                        prefixes or glob names for Python test class discovery\n",
      "  python_functions (args):\n",
      "                        prefixes or glob names for Python test function and\n",
      "                        method discovery\n",
      "  disable_test_id_escaping_and_forfeit_all_rights_to_community_support (bool):\n",
      "                        disable string escape non-ascii characters, might cause\n",
      "                        unwanted side effects(use at your own risk)\n",
      "  console_output_style (string):\n",
      "                        console output: \"classic\", or with additional progress\n",
      "                        information (\"progress\" (percentage) | \"count\").\n",
      "  xfail_strict (bool):  default for the strict parameter of xfail markers when\n",
      "                        not given explicitly (default: False)\n",
      "  enable_assertion_pass_hook (bool):\n",
      "                        Enables the pytest_assertion_pass hook.Make sure to\n",
      "                        delete any previously generated pyc cache files.\n",
      "  junit_suite_name (string):\n",
      "                        Test suite name for JUnit report\n",
      "  junit_logging (string):\n",
      "                        Write captured log messages to JUnit report: one of\n",
      "                        no|log|system-out|system-err|out-err|all\n",
      "  junit_log_passing_tests (bool):\n",
      "                        Capture log information for passing tests to JUnit\n",
      "                        report:\n",
      "  junit_duration_report (string):\n",
      "                        Duration time to report: one of total|call\n",
      "  junit_family (string):\n",
      "                        Emit XML for schema: one of legacy|xunit1|xunit2\n",
      "  doctest_optionflags (args):\n",
      "                        option flags for doctests\n",
      "  doctest_encoding (string):\n",
      "                        encoding used for doctest files\n",
      "  cache_dir (string):   cache directory path.\n",
      "  log_level (string):   default value for --log-level\n",
      "  log_format (string):  default value for --log-format\n",
      "  log_date_format (string):\n",
      "                        default value for --log-date-format\n",
      "  log_cli (bool):       enable log display during test run (also known as \"live\n",
      "                        logging\").\n",
      "  log_cli_level (string):\n",
      "                        default value for --log-cli-level\n",
      "  log_cli_format (string):\n",
      "                        default value for --log-cli-format\n",
      "  log_cli_date_format (string):\n",
      "                        default value for --log-cli-date-format\n",
      "  log_file (string):    default value for --log-file\n",
      "  log_file_level (string):\n",
      "                        default value for --log-file-level\n",
      "  log_file_format (string):\n",
      "                        default value for --log-file-format\n",
      "  log_file_date_format (string):\n",
      "                        default value for --log-file-date-format\n",
      "  log_auto_indent (string):\n",
      "                        default value for --log-auto-indent\n",
      "  faulthandler_timeout (string):\n",
      "                        Dump the traceback of all threads if a test takes more\n",
      "                        than TIMEOUT seconds to finish.\n",
      "  addopts (args):       extra command line options\n",
      "  minversion (string):  minimally required pytest version\n",
      "  required_plugins (args):\n",
      "                        plugins that must be present for pytest to run\n",
      "  render_collapsed (bool):\n",
      "                        Open the report with all rows collapsed. Useful for very\n",
      "                        large reports\n",
      "  max_asset_filename_length (string):\n",
      "                        set the maximum filename length for assets attached to\n",
      "                        the html report.\n",
      "  qs_enabled (bool):    default value for --qase\n",
      "  qs_api_token (string):\n",
      "                        default value for --qase-api-token\n",
      "  qs_project_code (string):\n",
      "                        default value for --qase-project\n",
      "  qs_testrun_id (string):\n",
      "                        default value for --qase-testrun\n",
      "  qs_testplan_id (string):\n",
      "                        default value for --qase-testplan\n",
      "  qs_new_run (bool):    default value for --qase-new-run\n",
      "  qs_complete_run (bool):\n",
      "                        default value for --qase-complete-run\n",
      "  qs_debug (bool):      default value for --qase-debug\n",
      "\n",
      "environment variables:\n",
      "  PYTEST_ADDOPTS           extra command line options\n",
      "  PYTEST_PLUGINS           comma-separated plugins to load during startup\n",
      "  PYTEST_DISABLE_PLUGIN_AUTOLOAD set to disable plugin auto-loading\n",
      "  PYTEST_DEBUG             set to enable debug tracing of pytest's internals\n",
      "\n",
      "\n",
      "to see available markers type: pytest --markers\n",
      "to see available fixtures type: pytest --fixtures\n",
      "(shown according to specified file_or_dir or current dir if not specified; fixtures with leading '_' are only shown with the '-v' option\n",
      "warning : c:\\users\\ilia_serdiuk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyreadline\\py3k_compat.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  return isinstance(x, collections.Callable)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pip install pytest\n",
    "!pytest --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810eae41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simple test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de07ea12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parametrization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dea911",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conftest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed85f49c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9805a346",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Html report"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
